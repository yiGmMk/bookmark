---
layout: post
---
Title: 三千预算本地 70b 大模型 - V2EX

URL Source: https://www.v2ex.com/t/1102193

Published Time: 2025-01-03T01:56:21Z

Markdown Content:
首页 注册 登录
V2EX = way to explore
V2EX 是一个关于分享和探索的地方
现在注册
已注册用户请  登录
V2EX  ›  Local LLM
三千预算本地 70b 大模型
  8     nlzy · 3 小时 54 分钟前 · 1614 次点击
TL;DR
三张 MI50 显卡
E5 洋垃圾 / 山寨主板 / DDR3 RDIMM
服务器电源魔改供电 / 开放式机架
model	split	prefill	decode
Llama 3.3 70b q4_K_S	row	58.80 t/s	12.58 t/s
Qwen 2.5 72b q4_K_S	layer	75.72 t/s	9.85 t/s
QwQ Preview 32b q4_K_S	row	141.30 t/s	20.65 t/s
Dolphin Mistral Nemo 12b q8_0	none	482.98 t/s	35.92 t/s

整台主机（不含鼠标键盘显示器）加起来共 3140.19 元。

神奇的 MI 50

逛 reddit 的 LocalLLaMA 板块，发现一块很有意思的 GPU：由 AMD 在 2018 年底推出的 Instinct MI50 。与 Radeon VII 和 Radeon Pro VII 使用相同的 GPU 芯片，是面向数据中心的加速卡。台积电 7nm 工艺，单精算力 13 TFLOPS ，显存是 16G 的 HBM 2 ，带宽 1TB/s。众所周知，在单请求、无并发量的情况下，LLM 推理的性能是受限于显存带宽的，1TB/s 的显存带宽，就是推理速度的坚实基础（后面才发现我太天真了）。

这张卡比较有意思的地方在于，因为这张卡的显存带宽很高，以太坊挖矿算力非常猛，前几年矿潮的时候被矿老板大量买入挖矿，在国内市场上的量很多。而现如今矿潮退去，咸鱼上可以找到大量 MI50 ，在这段话写作的当下（ 2025 年 1 月），标价通常在 500 多不到 600 元一张。一般本地部署 70b 左右的模型，4 bit 量化后的模型权重需要使用大约 40G 的显存。也就是 1700 左右的价格买三张 MI50 ，三张显卡的显存刚好能装 70b 模型。

以及，虽说是数据中心卡，但是这张卡上面有一个 minidp 接口，而且这张卡可以刷 Radeon Pro VII 的 BIOS ，刷了之后就是一张 Radeon Pro VII 。我让卖家帮我刷了，倒不是为了玩游戏，而是因为 Radeon Pro VII 的 BIOS 有默认 190 W 的功耗限制，原版的 MI50 是 250 W 。这种垃圾矿卡，用的时候还是限制一下功耗来得安心点，把 BIOS 刷掉也省得用软件调整功耗了，也方便以后用来打游戏或者出给用来打游戏的垃圾佬。

另外一提，闲鱼上有很多这张卡的“假卡”，和“真卡”的外观几乎一样。两者的区别之一是，“假卡”的 PCB 上有空的地方，而“真卡”在对应位置是贴了元件的，可以从显卡背板上的孔看到。二是“真卡”的标签上写的是 AMD RADEON INSTINCT MI50 ，而“假卡”写的是 Radeon VII 。

在网上查资料看到两种说法，有人说，“假卡”是 AMD 专门整的挖矿的版本，也有人说，“假卡”是搬板魔改的。在咸鱼上，“假卡”一般会再便宜一点。当然，这两者铁定都是 100% 挖矿的矿渣，两者差价较大的情况下没必要追求“真卡”。另外，“真卡”刷 BIOS 也比较复杂，据说不能软件直接刷。“假卡”随便刷。

平台

我没有能插 PCIe 的电脑，所以我重新配了一台。

CPU 选择了 E5-2666V3 ，10 核心 20 线程，全核睿频能有 3.2 GHz ，在一众 E5 洋垃圾里算是比较高的了。选择它的另外一个原因是这颗 U 可以支持 DDR3 的内存。价格也便宜，淘宝 48 元就能包邮。

内存选择了 128G 插满。主要的原因是 32G DDR3 RDIMM 内存一根只需要 50 元，也就是说 64G 和 128G 只相差了 100 元，四舍五入约等于不要钱。内存大了缓存大，可以少读点盘。

主板就是经典 x99 寨板，3 条 PCIe 3.0 x16 的插槽刚好能插下三张显卡（其中两个插槽的速率是 x8 ）。没有什么多谈的。三百多，便宜。

选择了开放式机架，30 元，能用就行。

CPU 散热其实不必担心，E5-2666V3 虽然功耗挺高的，但因为 die 面积大，所以不像先进制程那样积热严重。我选择了下压式的散热，主要是担心寨厂用的低端 MOS 发热大，以及标准电压的 RDIMM 内存是个发热大户，用下压式散热可以带点风。50 元，略贵。

SSD 用了凯侠 XG6 1T ，PCIe 3.0 时代性能第一梯队的盘。全新盘，360 。

电源是最头痛的。三张 MI50 的功耗是 3*190W ，加上 CPU 的 120W ，加上平台的 100W 。总共加起来都 800W 了。MI50 刷了 BIOS 后的 190W 的功耗限制是软限制，实际跑起来的动态功率会超，按照 800W 给电源是不行的，肯定要给点冗余，那就至少是 1000W 以上了（毕竟是三卡平台）。

最后选择了全新 ATX 电源给平台供电，二手服务器电源给显卡供电的方案（反正显卡是二手矿渣，不配用正经电源）。感谢 DELL 更新过他们的服务器电源规范，似乎是旧款的电源不能用在新款服务器上，所以旧款的二手电源非常低廉。30 元就能买一个 750W 电源甚至还是 80 PLUS 白金，几乎是正经全新 ATX 白金电源的十分之一价格不到。显卡供电只需要 12v ，不需要 5V 和 3.3V ，也就不需要取电板了，可以直接焊线。

硬件总结
E5-2666V3
精粤 X99-TI D3 PLUS
三星 PC3L-10600R 4Rx4 32G (x4)
AMD Instinct MI50 (x3)
凯侠 XG6 1TB
玄武 500k ATX 电源
DELL 750W 服务器电源
开放式机架
MI50 散热风扇
下压式 CPU 风扇

总计 373.95 + 215.97 + 1730 + 357.89 + 145 + 40 + 33 + 143.98 + 59.9 + 40.5 = 3140.19 ，超支 140 元，基本达成三千预算的目标。

价格分别是：处理器主板套装，四条内存，三张显卡，固态硬盘，ATX 电源，服务器电源，机架，显卡散热，CPU 散热，显卡电源线，除显卡卖家给我发了个巨他爹贵的顺丰特快到付外，其他价格均含运费。

软件

装的 PVE 。本来想 PCIe 显卡直通，没成功，虚拟机里的显卡驱动会卡在初始化。于是改用 LXC 直通，需要在 PVE 上安装 ROCm 对应的 AMDGPU 内核驱动。

PVE 不在 AMD 的支持列表里面，驱动直接用 AMD 官方的 Ubuntu 24.04 的 apt 源就行，反正内核版本号差不多。添加 AMD 驱动的源之后，需要先手动安装 proxmox-headers-6.8 内核头文件，然后安装显卡驱动 amdgpu-dkms，需要直通给容器的设备文件有 /dev/kfd 和 /dev/dri/card* 和 /dev/dri/renderD*。

LXC 容器内的系统选择了 Ubuntu 24.04 。按照 AMD 的说明配置好 ROCM 的源之后在容器里安装 rocm 这个包就行，我安装的版本是 ROCm 6.2.4 。

llama.cpp 可以直接按照文档编译过。编译的时候开启 GGML_CUDA_FORCE_CUBLAS，在 MI50 上的 prefill 性能会快一些。vLLM 编译一堆问题，换用 rocm/vllm 后又改了一堆东西才能编译过，完了才发现 vLLM 在 AMDGPU 上不支持 4bit 量化，告辞。TGI 写明了只支持 MI250 以上的显卡。告辞。TensorRT LLM 想都不敢想，告辞。

推理

所以软件就只能用 llama.cpp ，即使很多人说它优化不好，毕竟它能用。需要说明的是，以下 MI50 的性能表现是我改过 llama.cpp 中的代码 mmvq.cu 中的 nwraps 和 rows_per_cuda_block 后才能达到的，不然结果还会更差，并发度 B=2 相对 B=1 基本是负提升。

以下是 3 张 MI50 运行 70b 模型的炸显存现场：

model	size	split	KV	ctx	prefill	decode
Llama 3.3 70b q4_K_S	37.57 GiB	row	f16	2048	52.73 t/s	11.56 t/s
Llama 3.3 70b q4_K_S	37.57 GiB	row	f16	4096	-	BOOM
Llama 3.3 70b q4_K_S	37.57 GiB	layer	f16	8192	74.37 t/s	10.02 t/s
Llama 3.3 70b q4_K_S	37.57 GiB	layer	f16	16384	-	BOOM
Llama 3.3 70b q4_K_S	37.57 GiB	layer	q8_0	32768	70.36 t/s	9.84 t/s
Llama 3.3 70b q4_K_S	37.57 GiB	layer	q8_0	65536	-	BOOM
Qwen 2.5 72b q4_K_S	40.87 GiB	row	f16	1024	-	BOOM
Qwen 2.5 72b q4_K_S	40.87 GiB	layer	f16	4096	74.32 t/s	9.39 t/s
Qwen 2.5 72b q4_K_S	40.87 GiB	layer	f16	8192	-	BOOM
Liberated Qwen 1.5 72b q4_K_S	39.04 GiB	row	f16	1024	-	BOOM
Liberated Qwen 1.5 72b q4_K_S	39.04 GiB	layer	f16	2048	72.95 t/s	9.47 t/s
Liberated Qwen 1.5 72b q4_K_S	39.04 GiB	layer	f16	4096	-	BOOM

结论是”凑合能跑“。只有 Llama 3.3 使用 row 分割后上下文可以开到 2048 ，此时达到 11 t/s 的生成速度。其他模型使用 layer 分割后上下文可以满足日常使用，但是生成速度跌倒 9.x t/s 左右，这个速度显然不能让我满意。开启 flash attation 并使用 8 bit 量化的 KV cache 后，Llama 3.3 可以开到 32k 的上下文了，但是性能进一步下跌（是的，MI50 开启 llama.cpp 的 flash attation 后性能是下跌的），此时输入 32k 的上下文也需要将近 8 分钟，这种 32k 上下文基本没有使用价值。

model	PP	TG	B	prefill	decode
Llama 3.3 70b q4_K_S	256	128	1	74.46 t/s	10.26 t/s
Llama 3.3 70b q4_K_S	256	128	2	78.58 t/s	13.07 t/s
Llama 3.3 70b q4_K_S	256	128	4	76.52 t/s	12.13 t/s

以上是多请求并发处理的性能。结论是 MI50 在 70b 这个尺寸上，增加并发并带来的性能提升并不显著。在 B=4 的情况下就出现了负提升。

然后使用 2080 Ti 22G 魔改版与 MI50 一同比较，测 2080 Ti 的原因是除了 MI50 我就只有 2080 Ti 。2080 Ti 只有一张，跑不了 70b 模型，只能测小一些的模型。以下是 Dolphin Mistral Nemo 12b q8_0 的测试结果。Dolphin Mistral Nemo 是我非常喜欢的模型，使用“你是一个色〇小说写作工具”作为系统提示词，其生成的文字变态程度之高，相比其它开放权重大模型一骑绝尘，甚至 70b 的模型也不能望其项背。而它仅有 12b ，强烈推荐。

B	MI50 prefill	2080 Ti prefill	MI50 decode	2080Ti decode
1	493.28 t/s	1669.98 t/s	33.45 t/s	37.65 t/s
2	474.35 t/s	1660.32 t/s	49.12 t/s	70.63 t/s
4	442.91 t/s	1593.60 t/s	55.89 t/s	110.04 t/s
8	393.13 t/s	1446.54 t/s	47.56 t/s	128.36 t/s

以下是 QwQ Preview 32b q4_K_S 的测试结果。QwQ 也是我非常喜欢的模型，用 32b 的参数就做到了很强的推理能力，没事玩玩也挺好玩的。

B	MI50 prefill	2080 Ti prefill	MI50 decode	2080Ti decode
1	140.37 t/s	574.70 t/s	17.80 t/s	21.44 t/s
2	142.07 t/s	575.49 t/s	26.88 t/s	33.94 t/s
4	136.37 t/s	562.20 t/s	32.62 t/s	43.13 t/s
8	126.70 t/s	531.64 t/s	9.75 t/s	47.62 t/s

小模型测试中，12b 是 MI50 单卡跑的，32b 是双卡跑的。因为这个平台的 MI50 互联是走 PCIe 的，带宽和延迟都不是很好，结果是调用的显卡越多，生成的速度就越慢。

结论也没什么好说的，只要是 2080 Ti 能运行的模型，并行性能就一定会被 2080 Ti 暴打，不出意外。当然，1 张 2080 Ti 22G 的价格比 3 张 MI 50 还贵，那就是别的故事了。

微调

除了大模型推理，还试了一下小模型的微调。试了一下 LLaMA-Factory ，使用过程的体验令人崩溃（当然这不是 LLaMA-Factory 的问题）

基本上就是，运行的时候 XXX 报错，查阅 XXX 的手册后发现不支持 ROCm ，随后找到 github 上有 rocm 维护的分支，然后切到 rocm 维护的分支编译。再运行一遍，YYY 又报错了，于是如此循环再来一遍。最后卡在一个 hipblaslt 中的一个奇奇怪怪的报错，似乎是 MI50 太老了，连 ROCm 都不提供支持了。最终的结果是 DeepZero 跑不起来。

用不了 DeepZero 的结果就是，一张 2080 Ti 22G 单卡能跑起来的 7b QLoRA 微调，三张 MI50 居然还要降低 LoRA rank 才能跑得动。而更加令人崩溃的一点是，因为没有 Tensor Core ，这三张 MI50 跑 LoRA 的速度连一张 2080 Ti 的一半都比不过。再次无语。

结论

垃圾。真的垃圾。

一张卡的 16G 显存极其尴尬。刚好装不下一个 4-bit 的 32b 模型，QwQ 跑不了，qwen2.5 和 qwen2.5-coder 也跑不了 32b ，只能跑 14b 的。问题是 14b 的模型用高频双通道 DDR5 的 CPU 也勉强能跑了。

两张卡的 32G 的处境也好不到哪去。装不下一个 4-bit 的 70b 模型。占用了两个宝贵的 PCIe 16x 的插槽和 500W 的电源功率，却只能跑 32b 模型，非常鸡肋。

唯一值得玩的就是三张卡的 48G 显存，这个显存大小能跑 70b 的模型了，但是 10 左右的 token 生成速度体验真的不咋地。

除了 1TB/s 和 13 TFLOPS 的单精度浮点性能有点唬人之外，实际使用体验一团糟。这不支持，那也不支持。这优化不行，那优化也不行。单请求无并发下的大模型推理生成速度居然比不过带宽只有其 60% 的 2080 Ti 。计算密集的 prefill 和微调更是被 2080 Ti 打得满地找牙。

也战不了未来，因为 AMD 在 ROCm 的文档中早就将 MI50 标记为准备弃用了。ZLUDA 也跑不了，这东西一开始就不支持 GCN 架构的老卡。反观 NVIDIA ，2080 Ti 还比 MI50 早发布两个月呢，使用过程中就没遇到跑不了的软件，官方的 CUDA 套件也是连 2080 Ti 前代架构的前代架构都还在提供支持。

哎，AMD ，我该说点什么好。

还是说点什么

你不把自己的命给革了，就有人来替你革了你的命。MI50 的落后之处在于它所使用的 GCN 架构，这个架构的第一个版本发布于 2012 年初，是 MI50 发布的六年以前了。它也没有专有硬件应对 AI 在低精度密集矩阵运算上的算力饥渴，而是仍在使用 SIMT 。所以，即使它使用的是台积电 7nm 工艺，即使它使用的是 HBM 显存，即使他的 SIMT 算力和 2080 Ti 几乎同级，即使它已经有了 sdot4 sdot8 fdot2 指令用于加速低精度运算，但是这些都远远不够。于是在 MI50 发布后的第六年，就匆匆被 AMD 标记为了”弃用“。MI50 就是一张被其他架构革了命的显卡，令人唏嘘。

本文由人类（也可能是猫猫?）写成，没有使用任何 AI 辅助。

 MI50 70B 推理
27 条回复  •  2025-01-03 13:50:46 +08:00
		
    1
notsyncing      3 小时 43 分钟前
感谢分享，本来也在考虑 MI50 ，看来还是算了，老老实实等 B770😂
		
    2
Chihaya0824      3 小时 41 分钟前
非常好文章，让我避免踩坑
看了一圈感觉还是 v100 * 4 或者直接等今年 macstudio 完事了
		
    3
bigtear      3 小时 37 分钟前
技术贴，赞，折腾的快乐
		
    4
sunny352787      3 小时 34 分钟前
BOOM ！哈哈哈哈哈
		
    5
huipengly      3 小时 33 分钟前
折腾快乐
		
    6
mypchas6fans      3 小时 24 分钟前
我曾经想过海鲜市场的 MI100 ，smzdm 上看到一篇说画 SDXL 的，但还是放弃了，没有 LZ 研究这么深入
		
    7
LXchienne      3 小时 18 分钟前
赞行动和总结
		
    8
marquina      3 小时 13 分钟前
“高性价比”的背后就是折腾+不一定好用，太真实了
		
    9
zuotun      3 小时 7 分钟前
三千预算 70b ？第一反应是去买 API 。
		
    10
dishonest      3 小时 5 分钟前
细说一下“小说写作工具”的这个系统提示词
		
    11
wangshushu      3 小时 1 分钟前    5
看到标题，心里冒出一句“痴人说梦，他知不知道什么是 70b”，点开正文，“跪了，您比我懂大模型！”
		
    12
FlytoSirius      2 小时 53 分钟前
挺好玩的
		
    13
niubilewodev      2 小时 49 分钟前    1
你焊的这个线，让我害怕😨
		
    14
yangyaofei      2 小时 47 分钟前
本来想进来看乐子的, 没想到真有东西, 以为 200 买个树莓派剩下的买 api,哈哈哈哈

vLLM 是支持 ROCm 的, 不知道性能怎么样, 要是能上 4 块并上 65536 长度, 家用和开发就能凑合
		
    15
SamLacey      2 小时 37 分钟前
感谢分享
		
    16
topang      2 小时 30 分钟前
真是不错的分享，我这门外汉都看得有味
		
    17
x86      2 小时 22 分钟前
这店员看着害怕
		
    18
x86      2 小时 22 分钟前
店员 > 电源
		
    19
2067      2 小时 18 分钟前
技术贴点赞

难道只有我一个人关注到了“Dolphin Mistral Nemo 是我非常喜欢的模型”这一段吗，受益匪浅 
		
    20
ghwolf007      2 小时 15 分钟前
生命不息 折腾不止
		
    21
maichael      2 小时 12 分钟前
你这焊法怕是会物理层面上的 BOOM
		
    22
c9cc      1 小时 50 分钟前
好文。楼主 做了我想做的事情
		
    23
anrgct      57 分钟前
个人觉得 16k 上下文、200tok/s prefill 、15tok/s decode 是本地使用最低要求了
		
    24
MuhammadWang      49 分钟前
楼主动手能力真 NB.... 我还是 m4 pro 跑吧，内存常驻 14B 和 32B 模型，省心....
		
    25
lzgshsj      17 分钟前
感谢分享。显存确实量大管饱，就是实际使用的时候确实很难让人绷得住~
		
    26
zlo309618100      1 分钟前
Dolphin Mistral Nemo 是我非常喜欢的模型，使用“你是一个色〇小说写作工具”作为系统提示词，其生成的文字变态程度之高，相比其它开放权重大模型一骑绝尘，甚至 70b 的模型也不能望其项背。而它仅有 12b ，强烈推荐
已阅重点
		
    27
daweii      刚刚 via iPhone
666 生命不息，折腾不止。我选择调 API 🐶
关于   ·   帮助文档   ·   博客   ·   API   ·   FAQ   ·   实用小工具   ·   5407 人在线   最高记录 6679   ·      Select Language
创意工作者们的社区
World is powered by solitude
VERSION: 3.9.8.5 · 25ms · UTC 05:50 · PVG 13:50 · LAX 21:50 · JFK 00:50
Developed with CodeLauncher
♥ Do have faith in what you're doing.
word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word

mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1
mmMwWLliI0fiflO&1

